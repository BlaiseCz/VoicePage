# VoicePage

VoicePage is a strict, local-first voice interface for navigating and controlling a web page. It is designed for high precision and predictable behavior, not open-ended natural language understanding.

## What it is
- A deterministic voice router: you say a visible label, VoicePage resolves it to a specific DOM element and performs a safe default action.
- Local-first by default (v1): no required network calls; logs/metrics stay local.
- Split into a headless core engine and an optional UI overlay implemented as Web Components.

## What it is not (v1)
- Conversational intent parsing / LLM reasoning
- Form dictation or rich text entry
- Full cross-browser/mobile support (desktop Chrome/Firefox first)

## Core idea: “say what you see”
- Targets are computed from:
  - `data-voice-label` (canonical developer override), or
  - strict accessible/visible label fallback rules
- Matching is deterministic and conservative.
- Labels and transcripts are normalized to lowercase in v1.

## User flow (v1)
1) User enables listening (UI toggle; wake word optional later).
2) User says: `open` or `click`.
3) VoicePage captures a short utterance (VAD-terminated) and transcribes it.
4) VoicePage resolves the transcript to a unique target or a candidate set.
5) If unique, it highlights briefly and executes (typically click).
6) If ambiguous, it shows a disambiguation overlay (depending on `collisionPolicy`).
7) `stop` interrupts; `cancel` aborts/dismisses.

## Safety and interaction scope
- VoicePage does not guess when uncertain.
- Collision handling is configurable:
  - `collisionPolicy = "disambiguate"` (default): allow duplicates but require explicit selection.
  - `collisionPolicy = "error"`: treat duplicates as misconfiguration.
- Modal-first scope: if a blocking modal/popup is open, VoicePage only targets controls within the topmost modal/popup.

## Documentation
- [`docs/DEVELOPMENT.md`](docs/DEVELOPMENT.md) — **full setup, build, test, and training guide**
- [`docs/ARCHITECTURE.md`](docs/ARCHITECTURE.md) — system design, state machine, event contract
- [`docs/GOALS.md`](docs/GOALS.md) — scope, principles, success criteria
- [`docs/UI_WEB_COMPONENTS.md`](docs/UI_WEB_COMPONENTS.md) — UI component specs
- [`docs/LABELING_SPEC.md`](docs/LABELING_SPEC.md) — DOM labeling, normalization, collision rules

## Quick start

### Prerequisites
- **Node.js** >= 20 ([nvm](https://github.com/nvm-sh/nvm) recommended)
- **pnpm** >= 10 (`curl -fsSL https://get.pnpm.io/install.sh | sh -`)
- **Python** >= 3.9 (only needed for KWS model training)
- **Docker** (optional, for containerized deployment)

### One-command start
```bash
./run.sh
# → http://localhost:3000       (stub mode — no mic, simulated pipeline)
# → http://localhost:3000?mode=real  (real audio — requires trained models)
```

The `run.sh` script handles everything: checks Node/pnpm, installs dependencies, builds all packages, verifies models, and starts the Vite dev server.

```bash
./run.sh              # Dev server with hot-reload
./run.sh --build      # Production build + static preview (http://localhost:4173)
./run.sh --install    # Install dependencies only
```

### Or step by step
```bash
pnpm install          # Install dependencies
pnpm build            # Build all workspace packages
pnpm dev              # Start the demo (Vite dev server on port 3000)
```

---

## Full deployment guide

### Step 1: Clone and install
```bash
git clone <repo-url>
cd voicepage
pnpm install
```

### Step 2: Train keyword models (required for real audio mode)

The real audio pipeline uses custom openWakeWord ONNX models for 5 keywords: `open`, `click`, `stop`, `cancel`, `help`.

```bash
# Set up the Python training environment (one-time)
cd tools/openwakeword
chmod +x setup.sh
./setup.sh

# Download training data (room impulse responses, background audio, features)
source venv/bin/activate
python train.py setup

# Train all keywords (minimal mode — fast, ~5 min, good for testing)
cd ../..
bash tools/openwakeword/train.sh --minimal

# Or train all keywords (full mode — slower, ~1–2 hours, production quality)
bash tools/openwakeword/train.sh
```

Training produces these files in `models/kws/`:
```
models/kws/
├── melspectrogram.onnx      # Shared mel preprocessor (downloaded by setup.sh)
├── embedding_model.onnx     # Shared embedding backbone (downloaded by setup.sh)
├── open.onnx                # Keyword classifiers (generated by training)
├── click.onnx
├── stop.onnx
├── cancel.onnx
└── help.onnx
```

### Step 3: Copy models to the demo app
```bash
cp models/kws/*.onnx apps/demo-vanilla/public/models/kws/
```

> **Note:** `train.sh` exports models to `models/kws/`. The demo app serves static files from `apps/demo-vanilla/public/`, so models must be copied there.

### Step 4: Obtain VAD and ASR models

These are not trained locally — download them from their respective sources:

**Silero VAD** → `apps/demo-vanilla/public/models/vad/silero_vad.onnx`
```bash
# Download from silero-vad releases (v5.1.2)
wget -O apps/demo-vanilla/public/models/vad/silero_vad.onnx \
  https://github.com/snakers4/silero-vad/raw/v5.1.2/src/silero_vad/data/silero_vad.onnx
```

**Whisper Tiny** → `apps/demo-vanilla/public/models/whisper/`
```bash
# Download from Hugging Face (onnx-community/whisper-tiny)
wget -O apps/demo-vanilla/public/models/whisper/whisper-tiny-encoder.onnx \
  https://huggingface.co/onnx-community/whisper-tiny/resolve/main/onnx/encoder_model.onnx
wget -O apps/demo-vanilla/public/models/whisper/whisper-tiny-decoder.onnx \
  https://huggingface.co/onnx-community/whisper-tiny/resolve/main/onnx/decoder_model.onnx
wget -O apps/demo-vanilla/public/models/whisper/tokenizer.json \
  https://huggingface.co/onnx-community/whisper-tiny/resolve/main/tokenizer.json
```

### Step 5: Run the demo
```bash
./run.sh
```

Open `http://localhost:3000?mode=real` in **Chrome or Firefox** (desktop). Grant microphone access when prompted.

---

## Docker deployment

```bash
# Build and run with docker compose
docker compose up --build
# → http://localhost:3000
```

The Docker setup:
1. **Builds** all packages and the demo app in a Node.js stage
2. **Serves** the static production build via nginx with required COOP/COEP headers
3. Models in `apps/demo-vanilla/public/models/` are bundled into the image

To rebuild after model or code changes:
```bash
docker compose up --build --force-recreate
```

---

## Important configuration notes

### Cross-Origin headers (required for real audio)
Real audio mode uses `SharedArrayBuffer` for ONNX Runtime WASM threading. The server **must** send:
```
Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Embedder-Policy: require-corp
```
- **Dev mode**: Vite sets these automatically (see `vite.config.ts`)
- **Docker**: The `nginx.conf` includes them
- **Custom deployment**: Add these headers to your server configuration

### Demo modes
| URL | Mode | Description |
|---|---|---|
| `http://localhost:3000` | Stub | No microphone, simulated engines. Type labels in the test panel. |
| `http://localhost:3000?mode=real` | Real Audio | Live microphone → KWS → VAD → ASR → action. Requires all ONNX models. |

### How to use the demo
1. Click the **mic indicator** (bottom-right) or **"Start Listening"** to enable listening.
2. In **stub mode**: type a label in the test panel (e.g. `submit`, `billing`, `analytics`) and click **"Simulate Open"**.
3. In **real mode**: say a keyword (`open`, `click`) followed by a visible element label.
4. Watch the **Event Log** for a step-by-step trace of every engine event.
5. Matched elements are highlighted and clicked automatically.
6. For ambiguous matches, a disambiguation modal appears.
7. Say `stop` to interrupt or `cancel` to dismiss.

---

## Project structure
```
packages/voicepage-core/   # Engine, state machine, DOM indexer, matcher, events
packages/voicepage-ui/     # Web Components overlay (indicator, modal, highlight)
apps/demo-vanilla/         # Vite + vanilla TS demo page
tools/openwakeword/        # KWS model training (setup.sh, train.sh, train.py, configs/)
models/kws/                # Trained ONNX models (gitignored)
docs/                      # Architecture, goals, labeling spec, UI spec, dev guide
```

## Documentation
- [`docs/DEVELOPMENT.md`](docs/DEVELOPMENT.md) — full setup, build, test, and training guide
- [`docs/ARCHITECTURE.md`](docs/ARCHITECTURE.md) — system design, state machine, event contract
- [`docs/GOALS.md`](docs/GOALS.md) — scope, principles, success criteria
- [`docs/UI_WEB_COMPONENTS.md`](docs/UI_WEB_COMPONENTS.md) — UI component specs
- [`docs/LABELING_SPEC.md`](docs/LABELING_SPEC.md) — DOM labeling, normalization, collision rules

## CI/CD
- **CI** (`.github/workflows/ci.yml`): build, typecheck, lint, security audit on every push/PR
- **Deploy** (`.github/workflows/deploy-demo.yml`): deploys demo to GitHub Pages on push to `main`

## Security
- `.gitignore` covers keys, secrets, credentials, model binaries, and audio datasets
- `.env.example` documents optional env vars — copy to `.env` (gitignored) for local use
- CI includes a secret scanner that fails if common API key patterns are found in source

## Status
Early-stage prototype. APIs and specs are expected to change.